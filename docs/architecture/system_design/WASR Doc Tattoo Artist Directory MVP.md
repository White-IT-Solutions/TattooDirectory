# High Priority

| Security Check | Difficulty | How to evaluate | Relevance to Our Architecture |
| :---- | :---- | :---- | :---- |
| **Enforce HTTPS Everywhere ⭐** \=Encryption of all database connections Use TLS for all traffic. Include HSTS to prevent downgrades.  | Easy | **Verify:** Attempt to access the site via HTTP and ensure it redirects to HTTPS. Use an SSL scanner (e.g. Qualys SSL Labs) to check certificate validity and configuration. Make sure HSTS header is present to enforce future HTTPS. | **Highly Relevant.** This is enforced by configuring the Amazon CloudFront distribution's Viewer Protocol Policy to "Redirect HTTP to HTTPS" and setting an HSTS header via a response headers policy. |
| **Secure Session Cookies** Set HttpOnly, Secure, and SameSite flags on cookies. This prevents JS access and cross-site leaks. | Easy | **Verify:** Inspect cookies in browser dev tools to confirm `HttpOnly` and `Secure` flags, and a SameSite attribute (Lax or Strict). Ensure cookies aren’t sent over HTTP or accessible via `document.cookie`. Tools like [**securityheaders.com**](http://securityheaders.com) can audit cookie flags. | **Not Applicable for MVP.** The MVP has no user authentication or sessions. This becomes CRITICAL when the "Claim Profile" feature is implemented, requiring a session management strategy. |
| **Prevent SQL Injection** Use parameterized queries or an ORM which auto-parameterizes. Never concatenate untrusted input into queries. | Moderate | **Verify:** Review database code for use of prepared statements or ORM query methods (no raw SQL string building). Manually test inputs with SQL meta-characters (like `' OR '1'='1`) to ensure they don’t succeed or break the app. Use tools like **sqlmap** in a safe environment to probe for SQL injection vulnerabilities. | **Not Applicable (but conceptually relevant).** The architecture uses DynamoDB and OpenSearch, which are not vulnerable to *traditional* SQLi. The relevant risk is NoSQL Injection in DynamoDB or Query Injection in OpenSearch. This is mitigated by using the AWS SDKs correctly, which parameterize requests, and by not constructing query strings from raw user input. |
| **Validate & Sanitize User Inputs ⭐** Enforce strict server-side validation (type, length, format) and sanitize outputs. This thwarts malicious inputs. \= Validating data before saving it in the database | Moderate | **Verify:** Check that all form inputs and API requests are validated on the backend (e.g., proper email format, numeric ranges). Try submitting out-of-range values or HTML/JS tags in inputs, the data should be rejected or neutralized.  Use libraries or frameworks’ validation features to simplify this. Look for output encoding in templates (to prevent rendering of injected scripts). | **Highly Relevant**. All query parameters sent to the API Gateway (e.g., `style`, `location` in `/v1/artists`) must be strictly validated in the `SearchArtists` Lambda function before being passed to OpenSearch to prevent query errors or injection. |
| **Protect Against XSS** Escape/encode all dynamic content in the UI and avoid dangerous JS APIs. Consider a Content Security Policy for defense-in-depth. | Moderate | **Verify:** Identify where user-provided data is displayed in the app and ensure it’s properly HTML-encoded. Test by inputting a sample `<script>alert(1)</script>` in form fields or URL params and confirm it doesn’t execute (it should render harmlessly or be blocked). Use browser dev tools or **OWASP ZAP** to simulate XSS payloads. Additionally, implement a strict **Content Security Policy** and check the browser console for any CSP violation reports (adjust sources as needed). | **Highly Relevant.** This is primarily a frontend concern. The Next.js application must correctly sanitize any data it renders, especially artist bios scraped from public sources. A strict CSP is an excellent defense-in-depth measure. |
| **Implement CSRF Protections** Use anti-CSRF tokens or SameSite cookies to block cross-site request forgeries. | Moderate | **Verify:** If your app uses cookie-based authentication, ensure you have CSRF tokens in forms or APIs (e.g. hidden form fields or header tokens) and that the server validates them. Alternatively, confirm that `SameSite=Lax/Strict` on auth cookies is set to mitigate CSRF. Test by trying to submit a form from a third-party domain (or using a tool like **Postman** without the token) – the action should be rejected if CSRF protection is working. | **Not Applicable for MVP.** CSRF attacks target state-changing actions (e.g., changing a password). The MVP is read-only for users and has no authenticated sessions. This becomes critical for the "Claim Profile" feature. |
| **Secure Authentication** ⭐ Use strong password hashing on the backend (e.g. bcrypt) and enforce a strong password policy (min length, complexity). No default or hard-coded creds. Implement 2FA for admin or sensitive accounts. | Moderate | **Verify:** Inspect the code or configuration to ensure passwords are never stored in plain text or reversible encryption – only as salted hashes (bcrypt/scrypt).  Try a very weak password to see if the policy rejects it (if applicable).  Ensure any default admin credentials are changed/removed. If two-factor auth is enabled, test the enrollment and login process. | **Not Applicable for MVP.** There are no user accounts. The architectural equivalent is securing service-to-service communication, which is correctly handled via short-lived, temporary credentials using IAM Roles. |
| **Ensure Proper Authorization** ⭐ Enforce role/permission checks on every API endpoint and page. Each request should be authenticated and authorized for that user’s data. | Moderate | **Verify:** For each sensitive action or data fetch, confirm the server is checking the user’s identity/roles (not relying on just front-end controls).  Try accessing resources belonging to another user (using IDs or URLs you shouldn’t have) to ensure the server returns “Forbidden” or no data. Consider writing unit/integration tests for authorization rules, and use tools like **Postman** or **Burp Suite** to manipulate requests (e.g., change an account ID in an API call) – the request should be denied. | **Critically Relevant (for services).** While there's no user authorization, the Principle of Least Privilege is strictly enforced via IAM. For example, the `GetArtistProfile` Lambda has a role allowing it to `dynamodb:GetItem` from the main table, but not to write data or access other services. |
| **Keep Software Updated** Regularly update dependencies, libraries, and server packages to patch known vulnerabilities. | Easy | **Verify:** Enable dependabot or run `npm audit`/`yarn audit` (for Node), `pip audit` (for Python), etc., to find vulnerable packages.  Update to the latest security patches for your OS and frameworks. Use **npm/yarn** audit reports or a tool like **Snyk** to ensure no high-severity vulnerabilities remain. This is an ongoing check – establish a schedule (e.g. monthly) to review and update critical components. | **Highly Relevant.** This applies to: 1\) Node.js dependencies (`npm audit`), 2\) The base Docker image for Fargate scrapers (should be scanned with Amazon ECR scanning), and 3\) The AWS Lambda managed runtime (should be set to the latest supported Node.js version). |
| **Secure File Handling** Public buckets and unauthenticated URLs can leak user data. Using signed URLs ensures only authorized users get access for a limited time. | Moderate | Are uploaded files stored in S3 (or equivalent like Cloudflare R2, Backblaze, GCS)? Does your DB store the file ownership/permissions correctly? Is your API verifying user auth before returning file access? Are you generating signed URLs (e.g., presigned S3 URLs) with limited validity? **Tools to Help:** AWS S3 SDK (`getSignedUrl`) Supabase Storage \+ RLS Cloudflare R2 with token authUse Postman to test file access flow (e.g. no auth \= no file) | **Not Applicable.** The application does not handle user file uploads. The S3 bucket for the frontend is correctly secured via an Origin Access Identity (OAI). |

# Medium Priority

| Security Check | Difficulty | How to evaluate | Relevance to Our Architecture |
| :---- | :---- | :---- | :---- |
| **Apply Content Security Policy** Deploy a strict CSP header to restrict sources of scripts, styles, etc.. This mitigates XSS and data injection risks. | Advanced | **Verify:** Define a `Content-Security-Policy` header that whitelists trusted domains for resources (scripts, fonts, APIs). Test your site with the CSP in a browser – open the console to see if any CSP violations occur (adjust policy until all legitimate content passes). You can use **report-uri** or browser dev tools to identify what would be blocked. Tools like **Mozilla Observatory** can validate your CSP. | **Highly Relevant.** This should be implemented within the Next.js application and delivered via CloudFront response headers policy. It's a powerful way to mitigate XSS and data injection risks. |
| **Set Security Headers** Beyond CSP, ensure headers like HSTS, X-Frame-Options, X-Content-Type-Options, etc., are in place. HSTS ensures only HTTPS; X-Frame-Options prevents clickjacking; X-Content-Type-Options stops MIME sniffing, etc. | Easy | **Verify:** Use a header-check tool (e.g. **securityheaders.com** or **curl**) to inspect headers. Confirm `Strict-Transport-Security` is present (for HSTS preload), `X-Frame-Options: DENY` or `SAMEORIGIN` is set, `X-Content-Type-Options: nosniff` is set, and `Referrer-Policy` is set (to avoid leaking referrers). These can often be added via a web framework setting or a reverse proxy configuration. | **Highly Relevant.** Like the CSP, these are low-effort, high-impact security controls that should be configured in CloudFront to be sent with every response. |
| **Limit Network Exposure** ⭐ \=Setting authentication for the database and making sure it is strong Only expose necessary services/ports to the public. Keep databases, admin panels, etc., on internal networks or behind VPN. | Moderate | **Verify:** Audit your cloud security groups or firewall rules – ports 80/443 should be public; databases (e.g. port 5432/3306) and internal services should be **restricted to the app server or VPN**. Use a port scanner like **Nmap** from an external network to ensure no unintended open ports. If an admin interface exists, consider IP whitelisting or VPN access only. | **Critically Relevant & Well-Handled.** The architecture excels here. Placing Lambda functions, Fargate tasks, and the OpenSearch cluster in private subnets is the direct implementation of this principle. The NAT Gateway provides controlled outbound access while preventing inbound connections.  |
| **DDoS and Basic WAF Protection** Use a service like Cloudflare (CDN with DDoS protection) or AWS Shield, and enable basic web application firewall rules to filter malicious traffic. | Easy | **Verify:** Enable the protection service and configure DNS through it (e.g., Cloudflare proxy enabled). Perform a rudimentary load test or use online tools to send bursts of traffic and observe if the service mitigates it (without harming your app). Check the WAF dashboard for any blocked attacks (simulating a simple SQL injection or XSS in a query param can trigger WAF rules). Ensure your server’s real IP is hidden behind the service. | **Critically Relevant & Well-Handled.** The architecture correctly uses CloudFront (which includes AWS Shield Standard for DDoS protection) and attaches AWS WAF with managed rule sets.  |
| **Logging and Monitoring** Instrument your app to log important events (logins, failures, errors) and monitor these logs for anomalies. | Moderate | **Verify:** Ensure that sensitive actions (auth events, data access) produce log entries (with no sensitive info in logs). Set up a log aggregation tool or at least save logs to files that you can review.  Test by performing a few actions (e.g., failed login, successful login, data update) and then checking that those events are recorded in the log. Consider using a service like **Sentry**, **Datadog**, or open-source ELK stack for real-time alerting on suspicious patterns (e.g., many failed logins \= possible brute force). | **Highly Relevant.** The design correctly specifies CloudTrail for an audit trail of all API calls, CloudWatch Logs for application logs, and X-Ray for tracing. This provides the necessary visibility to detect and investigate security events. |
| **Regular Backups & Recovery Plan** ⭐ Maintain automated backups of databases and user data, and secure them. Plan for restore in case of data loss. | Easy | **Verify:** Configure daily/weekly backups (database dumps, etc.) and store them in a secure off-site location or cloud storage (with access controls). Periodically test restoring a backup to ensure the data is usable and integrity is intact. Check backup logs or timestamps to confirm they run consistently. This protects users’ data from loss or ransomware scenarios. | **Highly Relevant & Well-Handled.** The LLD specifies that DynamoDB Point-In-Time Recovery (PITR) is enabled. This is the best practice for backing up DynamoDB and allows for restoration to any second in the preceding 35 days.   |

# Low Priority

| Security Check | Difficulty | How to Evaluate | Relevance to Our Architecture |
| :---- | :---- | :---- | :---- |
| **Server Hardening** Lock down server settings and OS: disable unnecessary services, use least privilege for processes, and keep default credentials/configs removed. | Moderate | **Verify:** Perform an inventory of running services on your server/container – turn off or uninstall anything not needed for the web app. Ensure the web server runs under a non-root user with limited permissions. Use a vulnerability scanner (e.g. **OpenVAS** or **Nessus**) on your server to identify open ports and services; confirm only expected ones are accessible. Check that no default admin interfaces (database admin, etc.) are publicly reachable or using default passwords. | **Low Relevance.** The architecture is serverless-first (Lambda) and uses managed services (S3, DynamoDB), where AWS is responsible for hardening the underlying servers. The user's responsibility is limited to hardening the Docker container image for the Fargate scraper tasks by using a minimal base image and removing unnecessary packages. |
| **Safe Use of External Auth/OAuth** ⭐ If your app integrates with OAuth/OIDC or uses JWT, implement anti-forgery parameters. E.g. use the OAuth *state* param and OIDC *nonce* for login flows; use PKCE for public clients. For JWT, use strong secrets and don’t accept insecure algorithms | Moderate | **Verify:** If using third-party login (Google, etc.), ensure the `state` parameter is present in auth requests and validated on return (test by altering the state in the redirect URI – it should reject). For OIDC, ensure a `nonce` is included and verified to prevent replay. If using JWTs, inspect the token issuance: it should be signed with a robust secret (and ideally a strong algorithm like HS256 or RS256) – never use `alg: none`. Try using a JWT with an invalid signature or altered payload to confirm the backend rejects it. | **Not Applicable for MVP.** The MVP has no user authentication. This will become Critically Relevant for the future "Claim Profile" feature, which will likely use an external identity provider (e.g., sign-in with Google/Instagram) and require proper implementation of OAuth/OIDC flows. |
| **Isolate Application Domains** If hosting multiple apps, use separate domains/subdomains for each to prevent cookie or localStorage data from bleeding between apps. This containment reduces cross-site impact. | Easy | **Verify:** Check that your application’s domain is not shared with unrelated services. For example, don’t host user-facing and admin apps on the exact same domain if not needed. Ensure cookies have appropriate domain scope (default is current host). Try setting a cookie in one subdomain and confirm it’s not accessible in another. This item is mostly preventive design consideration for multi-app setups. | **Low Relevance for MVP.** This project is a single, self-contained application on one domain. The principle would become relevant if a separate application, such as an admin dashboard, were built on a subdomain in the future. |
| **Clear Sensitive Data on Logout** ⭐ On user logout, properly terminate the session and clear client-side data. For example, send a `Clear-Site-Data: "*"` header to wipe caches/storage, and invalidate the session token on the server. | Easy | **Verify:** Log in, then log out and ensure the session cookie is deleted or no longer valid (try using it again – it should be rejected by server). Check that after logout, you cannot use browser back button to get cached authenticated pages (the Clear-Site-Data header helps with this). You can use browser dev tools to confirm that caches and storage are cleared on logout. | **Not Applicable for MVP.** The system does not have user accounts or sessions, so there is no login/logout functionality. This becomes a mandatory requirement when implementing the "Claim Profile" feature. |
| **Periodic Security Testing/Audits** Before shipping and regularly after, conduct scans and audits. Use automated tools (like **OWASP ZAP** or **Burp Suite** in passive mode) to crawl your app for vulnerabilities, and/or engage in a manual review or third-party pen-test. | Advanced | **Verify:** Run an automated web vulnerability scan and review the report for issues like XSS, SQLi, outdated libraries, misconfigurations. Many issues described above can be double-checked with such tools. Additionally, consider code scanning tools (linters or SAST) for your language (e.g., ESLint security rules, Bandit for Python). Treat this as a “check-up” – for each release, allocate time to run scans or have a security-conscious peer review critical changes. | **Highly Recommended.** To elevate this as a portfolio piece, automated security scanning should be integrated into the CI/CD pipeline. This includes running `npm audit` for dependency scanning, using static analysis (SAST) tools to lint code for security issues, and potentially running dynamic scans (DAST) against the dev/staging environment. |

